# DeepLearningPaper 
This repo shows some classical deep-learning papers I read. :grinning:
| Title | Authors |Link|Algorithm|
| --- | --- |---|---|
| ImageNet Classification with Deep ConvolutionalNeural Networks | Alex Krizhevsky、Ilya Sutskever、Geoffrey E. Hinton |[原文链接](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)|AlexNet|
| VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION | Karen Simonyan, Andrew Zisserman|[原文链接](https://arxiv.org/pdf/1409.1556.pdf)|VGG|
|Going deeper with convolutions|Christian Szegedy,Wei Liu,Yangqing Jia,Pierre Sermanet, Scott Reed,Dragomir Anguelov|[原文链接](https://arxiv.org/pdf/1409.4842.pdf)|GoogLeNet|
| Deep Residual Learning for Image Recognition | Kaiming He、Xiangyu Zhang、Shaoqing Ren、 Jian Sun |[原文链接](https://arxiv.org/pdf/1512.03385.pdf)|ResNet| 
|MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications|Andrew G.Howard，Menglong Zhu Bo，Chen Dmitry Kalenichenko|[原文链接](https://arxiv.org/pdf/1704.04861.pdf)|MobileNetV1| 
|Aggregated Residual Transformations for Deep Neural Networks| Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, Kaiming He| [原文链接](https://arxiv.org/abs/1611.05431)|ResNext| 
|Improving Language Understanding by Generative Pre-Training| Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever|[原文链接](https://paperswithcode.com/paper/improving-language-understanding-by)|GPT-1| 
|Language Models are Unsupervised Multitask Learners| Alec Radford, Jeffery Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever|[原文链接](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)|GPT-2|
